{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RoKNH7o0bTR",
        "colab_type": "code",
        "outputId": "864d5e80-32ee-4335-af50-10ef0ad1e68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping as EarlyStopping\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.layers import Activation\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "\n",
        "file = open('lotr_data/lotr.txt','rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "#remove unwanted characters\n",
        "chars_to_remove = ['…', '‚', '’', '‘', 'ó', '»', 'µ', '®', '«', '¥', '¤', '¢', '}', '{', '—', '_', '=', ';', '~', '`',\n",
        "                   '#', '!', '\\\"', '#', '&', \"\\'\", '(', ')', ',', '-', '.', '/', ':', '*', '[', ']', '\\\\', '|', '?', '–']\n",
        "\n",
        "for char_to_remove in chars_to_remove:\n",
        "  text = text.replace(char_to_remove, \"\")\n",
        "  \n",
        "#case insensitive\n",
        "text = text.lower()\n",
        "\n",
        "#remove extra spaces\n",
        "text = ' '.join(text.split())\n",
        "\n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# split text into words\n",
        "words = text.split(\" \")\n",
        "print ('Amount of words: {}'.format(len(words)))\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "    \n",
        "# print('{')\n",
        "# for char in char2idx:\n",
        "#     print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "# print('}')\n",
        "  \n",
        "#encode text from characters to numbers  \n",
        "encoded = np.array([char2idx[ch] for ch in text])  \n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "sequence_length = 100\n",
        "examples_per_epoch = len(encoded)//sequence_length\n",
        "\n",
        "# Create trainging examples\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded)\n",
        "\n",
        "# for i in char_dataset.take(5):\n",
        "#   print(idx2char[i.numpy()])\n",
        "  \n",
        "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "\n",
        "# #first 8 sentence batches\n",
        "# for item in sequences.take(8):\n",
        "#   print(repr(''.join(idx2char[item.numpy()])))\n",
        "  \n",
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# Transform each sequence into two sequences:\n",
        "# input(same as sequence), target (shifted by one index)\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print(\"Step {}\".format(i))\n",
        "  print(\"Input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "  print(\"Expected output: {} ({:s})\\n\".format(target_idx, repr(idx2char[target_idx] )))\n",
        "\n",
        "batch_size = 64\n",
        "steps_per_epoch = examples_per_epoch//batch_size\n",
        "\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "print(*dataset.take(1))\n",
        "\n",
        "# Length of the vocabulary (amount of unique characters)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 2897578 characters\n",
            "Amount of words: 570370\n",
            "37 unique characters\n",
            "Input data:  'in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worm'\n",
            "Target data: 'n a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms'\n",
            "Step 0\n",
            "Input: 19 ('i')\n",
            "Expected output: 24 ('n')\n",
            "\n",
            "Step 1\n",
            "Input: 24 ('n')\n",
            "Expected output: 0 (' ')\n",
            "\n",
            "Step 2\n",
            "Input: 0 (' ')\n",
            "Expected output: 11 ('a')\n",
            "\n",
            "Step 3\n",
            "Input: 11 ('a')\n",
            "Expected output: 0 (' ')\n",
            "\n",
            "Step 4\n",
            "Input: 0 (' ')\n",
            "Expected output: 18 ('h')\n",
            "\n",
            "(<tf.Tensor: id=2172, shape=(64, 100), dtype=int64, numpy=\n",
            "array([[28, 15, 11, ..., 28, 15,  0],\n",
            "       [16, 22, 25, ..., 30, 28, 11],\n",
            "       [ 0, 33, 11, ..., 25, 25, 30],\n",
            "       ...,\n",
            "       [ 0, 25, 24, ..., 29,  0, 30],\n",
            "       [11, 29,  0, ..., 16, 22, 11],\n",
            "       [14, 25, 33, ..., 13, 25, 11]])>, <tf.Tensor: id=2173, shape=(64, 100), dtype=int64, numpy=\n",
            "array([[15, 11, 30, ..., 15,  0, 30],\n",
            "       [22, 25, 28, ..., 28, 11, 14],\n",
            "       [33, 11, 29, ..., 25, 30, 18],\n",
            "       ...,\n",
            "       [25, 24, 22, ...,  0, 30, 25],\n",
            "       [29,  0, 22, ..., 22, 11, 23],\n",
            "       [25, 33, 24, ..., 25, 11, 30]])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UGziQ1LuOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/michalovsky/lotr_data.git"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}